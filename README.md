# Autoencoders
Autoencoders are unsupervised artificial neural networks for which the input is same as the output.They work by compressing the input into a latent representation and then reconstructing the output from this representation.


This notebook shows the implementation of following autoencoders:
- Fully connected autoencoder
- Sparse autoencoder
- Convolutional autoencoder
- Denoising (convolutional) autoencoder
- Sequence-to-sequence autoencoder
- Variational autoencoder

This notebook also covers some interesting practical applications of autoencoders such as **data denoising** and **dimensionality reduction for data visaulisation**.

### Data Denoising 
**input and output**

![Input](https://github.com/siddharth271101/Autoencoders/blob/master/images/autoencoders/denoising_scanned_documents_input.png)![Output](https://github.com/siddharth271101/Autoencoders/blob/master/images/autoencoders/denoising_scanned_documents_generated_output.png)

### Fashion MNIST visualization using an autoencoder followed by t-SNE
![Fashion_t-SNE](https://github.com/siddharth271101/Autoencoders/blob/master/images/autoencoders/fashion_mnist_visualization_plot.png)

### Fashion MNIST images generated by the variational autoencoder
<p align="center">
  <img width="460" height="300" src="https://github.com/siddharth271101/Autoencoders/blob/master/images/autoencoders/vae_generated_images_plot.png">
  </p>
